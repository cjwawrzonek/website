<p>The first step in constructing a narrative on how the network might be solving this task was to simply put the network into different activation states that it encounters during the task and then allow it to relax by updating activations until the network a) slows to below some threshold, b) begins bouncing between activation states, or c) exceeds some iteration cut off. Very early on, many time steps during both tasks appeared to be relaxing into an oscillatory behavior. To get a better idea of what was happeing in the network, we began looking for an appropriate subspace to plot the projection through time of the relaxation dynamics.
</p>

<p>First, we attempted to implement a simplified version of Mante and Sussillo’s dimensionality reduction through creating a regression subspace. We fit a new linear regression on a small set of continuous, independent task variables that would become our primary dimensions. The new regression was of the form:
</p> 

<p> $$ x_{ist} = \beta_{is0}\cos(\theta(ans_t)) + \beta_{is1}\sin(\theta(ans_t)) + \beta{is2}\cos(\theta(sample1_t)) + \beta_{is3}\sin(\theta(sample1_t))+ \beta{is4}\cos(\theta(sample2_t)) + \beta_{is5}\sin(\theta(sample2_t)) + \beta_{is6}select_t + \beta_{is7} $$ </p>

<p>Where xist is the firing rate of unit i at step s during trial t, anst is the answer location of trial t, sample1t is the sample 1 location of trial t, sample2t is the sample 2 location of trial t, and selectt is the selection cue of trial t (either 1 or −1). In their paper, Mante and Sussillo perform a variation of least squares regression to get the vector of beta weights, βvs, of length number of units. As seen earlier, this linear regression did not perform well in our case. Instead, we attempted to use the coefficients found from our LassoCV regression to form the vector of beta weights. Because they were working with noisy population neurons, Mante and Sussillo also applied a targeted dimensionality reduction through the first 12 principle components of the population z-scored responses. Given our model had no noise and only 25 − 36 hidden units, we skipped this step. Next, for each variable, we attempted to find the time step s that produced the most significant set of beta weights for the population, thus ”best” representing that variable. We used the max L2 norm of the beta weights. βmax = {β |s = argmax ||β ||2} (11) v vs svs2 Naturally, the time steps with the greatest norm for any given task parameter was the time step in which the task parameter was presented. The final step was orthogonalizing the regression subspace using QR decomposition. Bmax =QR=[Bmax,Bmax,Bmax,Bmax,Bmax,Bmax,Bmax] (12) 0123456 We attempted to project activations into this space with little success. We also attempted to use the non-orthogonalzed subspace, βv as well, with equally poor results. There are few reasons why this space was likely unhelpful in representing our activations. 1) Mante and Sussillo were working with unrelated task variables (color and motion) and orthogonalizing their regression weights didn’t effect the representations. In our task, it is very possible that the features are simply not orthogonal or that there is high multicollinear- ity between representations of, say sample 1 and sample 2. As we will soon see with future analyses, this certainly seems to be the case. 2) The task in Mante and Sussillo’s paper (information integration task) was a relatively stable task from start to finish. This led to a highly stable regression subspace as a basis for the data. In our model however, the task is highly dynamic. At each phase, it is very likely that the network is completely shifting its representation space. Performing an analysis similar to Mante and Sussillo, neither the orthogonal regression space or the straight beta vector stayed stable through different phases of the trial. We instead moved to using simple principle components as dimensions to reduce the activation space. While observing relaxation patterns of in the selection model, it became clear that the selection cue was not choosing one representation in working memory and collapsing the other, as hypothesized. Instead, the selection cue appeared to be acting as a timing mechanism that stopped an oscillation over some representation space. For example, Figure 13 shows a typical selection trial, only the selection cue was shifted forward one time step.
</p>